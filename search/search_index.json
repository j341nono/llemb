{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#llemb-unified-embedding-extraction-from-decoder-only-llms","title":"llemb: Unified Embedding Extraction from Decoder-only LLMs","text":"<p>llemb is a lightweight framework designed to extract high-quality sentence embeddings from Decoder-only Large Language Models (LLMs) like Llama, Mistral, and others. It unifies various state-of-the-art pooling strategies and efficiency optimizations into a simple, coherent interface.</p> <p>With <code>llemb</code>, you can easily leverage powerful LLMs for embedding tasks using advanced techniques like PromptEOL and PCoTEOL, with built-in support for Batch Processing and vLLM for high-throughput inference.</p> <p>Origin of the Name llemb is a coined name derived from LLM and Embedding. In Japanese pronunciation, the letter M in LLM is read as /\u025bm/ (emu), and Embedding begins with Em, which is pronounced /\u025bm/ as well. Overlapping this sound reflects the idea of merging LLMs and embeddings.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Flexible Backends: Seamless support for Hugging Face Transformers and vLLM (for high-speed inference).</li> <li>Advanced Pooling &amp; Prompting:<ul> <li>Pooling Methods: <code>mean</code>, <code>last_token</code>, <code>eos_token</code></li> <li>Prompt Templates: <code>prompteol</code>, <code>pcoteol</code> (Pretended Chain of Thought), <code>ke</code> (Knowledge Enhancement)</li> <li>Independent Control: Combine any pooling method with any prompt template</li> </ul> </li> <li>High-Performance:<ul> <li>Batch Processing: Efficiently handles large datasets with automatic CPU offloading and progress bars (<code>tqdm</code>).</li> <li>Quantization: Native support for 4-bit and 8-bit via <code>bitsandbytes</code> (Transformers) or <code>fp8</code>/<code>awq</code>/<code>gptq</code> (vLLM).</li> </ul> </li> <li>Granular Control: Extract embeddings from any layer (defaults to recommended layers based on research).</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install via PyPI using <code>pip</code> or <code>uv</code>.</p> <p>Basic Installation (includes quantization support)</p> <pre><code>pip install llemb\n# or\nuv add llemb\n</code></pre> <p>Starting from v0.2.2, <code>bitsandbytes</code> for quantization is included by default.</p> <p>With vLLM Support</p> <p>To enable the vLLM backend for faster inference:</p> <pre><code>pip install \"llemb[vllm]\"\n# or\nuv add llemb[vllm]\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Initialize the encoder and start extracting embeddings in just a few lines of code.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import llemb\n\n# 1. Initialize the encoder (defaults to auto-device detection)\nenc = llemb.Encoder(\"meta-llama/Llama-3.1-8B\")\n\n# 2. Extract embeddings using mean pooling\nembeddings = enc.encode(\"Hello world\", pooling_method=\"mean\")\n\nprint(embeddings.shape)\n# =&gt; (1, 4096)\n</code></pre>"},{"location":"#batch-processing-recommended-for-large-data","title":"Batch Processing (Recommended for Large Data)","text":"<p>Process lists of texts efficiently. <code>llemb</code> handles batching, progress tracking with <code>tqdm</code>, and memory management by automatically offloading computed embeddings to the CPU to prevent GPU VRAM saturation.</p> <pre><code>texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Machine learning is fascinating.\",\n    # ... thousands of texts ...\n]\n\n# Process in batches of 32\nembeddings = enc.encode(texts, batch_size=32, pooling_method=\"mean\")\n\nprint(embeddings.shape)\n# =&gt; (N, 4096)\n</code></pre>"},{"location":"#using-vllm-backend","title":"Using vLLM Backend","text":"<p>For maximum throughput on supported hardware, use the vLLM backend. You can pass vLLM-specific arguments (like <code>tensor_parallel_size</code> or <code>gpu_memory_utilization</code>) directly to the encoder.</p> <pre><code>enc = llemb.Encoder(\n    \"meta-llama/Llama-3.1-8B\",\n    backend=\"vllm\",\n    tensor_parallel_size=1,       # Number of GPUs\n    gpu_memory_utilization=0.9    # vLLM memory setting\n)\n\nembeddings = enc.encode(\"Hello vLLM\", pooling_method=\"last_token\")\n</code></pre>"},{"location":"#using-prompt-templates","title":"Using Prompt Templates","text":"<p>Leverage research-backed prompt templates to improve embedding quality. When you specify a template, <code>pooling_method</code> automatically defaults to <code>last_token</code>.</p> <pre><code>import llemb\n\nenc = llemb.Encoder(\"meta-llama/Llama-3.1-8B\")\n\n# Simple usage - pooling_method automatically defaults to last_token\nembeddings = enc.encode(\n    \"Hello world\",\n    prompt_template=\"prompteol\"\n)\n\n# Use PCoTEOL (Pretended Chain of Thought) template\n# Note: Automatically uses layer -2 when prompt_template is \"pcoteol\" or \"ke\"\nembeddings = enc.encode(\n    \"Hello world\",\n    prompt_template=\"pcoteol\"\n)\n\n# Or override the layer index\nembeddings = enc.encode(\n    \"Hello world\",\n    prompt_template=\"pcoteol\",\n    layer_index=-1  # Override default -2\n)\n</code></pre>"},{"location":"#advanced-usage-quantization","title":"Advanced Usage (Quantization)","text":"<p>Use quantization to reduce memory usage.</p> <pre><code>import llemb\n\n# Initialize with 4-bit quantization and force CUDA\nenc = llemb.Encoder(\n    model_name=\"meta-llama/Llama-3.1-8B\",\n    backend=\"transformers\",\n    device=\"cuda\",\n    quantization=\"4bit\"\n)\n\nembeddings = enc.encode(\n    \"Hello world\",\n    pooling_method=\"mean\",\n    prompt_template=\"ke\"  # Knowledge Enhancement template\n)\n</code></pre>"},{"location":"#configuration-optimization","title":"Configuration &amp; Optimization","text":"<p><code>llemb</code> passes arguments directly to the backend, allowing for deep customization.</p> <p>Using Flash Attention 2</p> <pre><code>import torch\n\nencoder = llemb.Encoder(\n    model_name=\"meta-llama/Llama-3.1-8B\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16\n)\n</code></pre> <p>Custom Quantization Config</p> <pre><code>from transformers import BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nencoder = llemb.Encoder(\n    model_name=\"meta-llama/Llama-3.1-8B\",\n    quantization_config=bnb_config\n)\n</code></pre>"},{"location":"#supported-pooling-methods-prompt-templates","title":"Supported Pooling Methods &amp; Prompt Templates","text":""},{"location":"#pooling-methods-pooling_method-parameter","title":"Pooling Methods (<code>pooling_method</code> parameter)","text":"Method Description Default Layer <code>mean</code> Average pooling of all tokens (excluding padding). -1 (Last) <code>last_token</code> Vector of the last generated token. -1 (Last) <code>eos_token</code> Vector corresponding to the EOS token position. -1 (Last)"},{"location":"#prompt-templates-prompt_template-parameter","title":"Prompt Templates (<code>prompt_template</code> parameter)","text":"Template Description Default Layer <code>prompteol</code> Simple prompt template targeting the last token. -1 (Last) <code>pcoteol</code> \"Pretended Chain of Thought\" - wraps input in a reasoning template. -2 <code>ke</code> \"Knowledge Enhancement\" - wraps input in a context-aware template. -2 <p>You can combine any pooling method with any prompt template. When using <code>pcoteol</code> or <code>ke</code> templates, the default layer is automatically set to -2 unless explicitly overridden.</p> <p>Smart Defaults (v0.2.2+): - When <code>prompt_template</code> is specified, <code>pooling_method</code> automatically defaults to <code>last_token</code> - When no <code>prompt_template</code> is specified, <code>pooling_method</code> defaults to <code>mean</code> - Explicit values always take precedence</p>"},{"location":"#migration-guide","title":"Migration Guide","text":"<p>If you're upgrading from an earlier version of <code>llemb</code>, the API has been refactored to separate pooling methods and prompt templates into orthogonal parameters.</p>"},{"location":"#breaking-changes-in-v022","title":"Breaking Changes in v0.2.2","text":"<ol> <li>API Refactoring: <code>pooling</code> parameter split into <code>pooling_method</code> and <code>prompt_template</code></li> <li>Smart Defaults: <code>pooling_method</code> automatically set to <code>last_token</code> when using templates</li> <li>Dependencies: <code>bitsandbytes</code> is now a core dependency (no longer optional)</li> <li>Removed: <code>index</code> pooling strategy has been removed</li> </ol> <p>Old API (deprecated):</p> <pre><code># Old: pooling parameter mixed strategies and templates\nenc.encode(\"text\", pooling=\"mean\")\nenc.encode(\"text\", pooling=\"pcoteol\")  # Mixed template + pooling\nenc.encode(\"text\", pooling=\"index\", token_index=0)  # Index strategy\n</code></pre> <p>New API (v0.2.2+):</p> <pre><code># New: separate parameters with smart defaults\nenc.encode(\"text\", pooling_method=\"mean\")\nenc.encode(\"text\", prompt_template=\"pcoteol\")  # pooling_method auto-set to last_token\n# Index strategy has been removed\n</code></pre>"},{"location":"#migration-examples","title":"Migration Examples","text":"Old Code New Code (Explicit) New Code (Smart Default) <code>enc.encode(text, pooling=\"mean\")</code> <code>enc.encode(text, pooling_method=\"mean\")</code> <code>enc.encode(text)</code> <code>enc.encode(text, pooling=\"last_token\")</code> <code>enc.encode(text, pooling_method=\"last_token\")</code> \u2014 <code>enc.encode(text, pooling=\"eos_token\")</code> <code>enc.encode(text, pooling_method=\"eos_token\")</code> \u2014 <code>enc.encode(text, pooling=\"prompteol\")</code> <code>enc.encode(text, pooling_method=\"last_token\", prompt_template=\"prompteol\")</code> <code>enc.encode(text, prompt_template=\"prompteol\")</code> <code>enc.encode(text, pooling=\"pcoteol\")</code> <code>enc.encode(text, pooling_method=\"last_token\", prompt_template=\"pcoteol\")</code> <code>enc.encode(text, prompt_template=\"pcoteol\")</code> <code>enc.encode(text, pooling=\"ke\")</code> <code>enc.encode(text, pooling_method=\"last_token\", prompt_template=\"ke\")</code> <code>enc.encode(text, prompt_template=\"ke\")</code> <code>enc.encode(text, pooling=\"index\", token_index=0)</code> Removed - use <code>last_token</code> or implement custom logic \u2014"},{"location":"#benefits-of-the-new-api","title":"Benefits of the New API","text":"<ol> <li>Orthogonality: You can now combine any pooling method with any prompt template.</li> <li>Clarity: The separation makes it clear what each parameter does.</li> <li>Flexibility: Easier to experiment with different combinations.</li> <li>Smart Defaults: </li> <li><code>pooling_method</code> automatically set to <code>last_token</code> when using templates</li> <li>Layer indices automatically set based on prompt template (can be overridden)</li> <li>Simpler Usage: Less boilerplate for common use cases with templates.</li> </ol>"},{"location":"#development","title":"Development","text":"<p>Clone the repository and sync dependencies using <code>uv</code>:</p> <pre><code>git clone https://github.com/j341nono/llemb.git\ncd llemb\nuv sync --all-extras --dev\n</code></pre> <p>Run Tests</p> <pre><code>uv run pytest\n</code></pre> <p>Static Analysis</p> <pre><code>uv run ruff check src\nuv run mypy src\n</code></pre>"},{"location":"#citations","title":"Citations","text":"<p>If you use the advanced pooling strategies implemented in this library, please cite the respective original papers:</p> <p>PromptEOL:</p> <pre><code>@inproceedings{jiang-etal-2024-scaling,\n    title = \"Scaling Sentence Embeddings with Large Language Models\",\n    author = \"Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2024\",\n    year = \"2024\"\n}\n</code></pre> <p>PCoTEOL and KE:</p> <pre><code>@article{zhang2024simple,\n    title={Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models},\n    author={Zhang, Bowen and Chang, Kehua and Li, Chunping},\n    journal={arXiv preprint arXiv:2404.03921},\n    year={2024}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is open source and available under the Apache-2.0 license.</p>"}]}